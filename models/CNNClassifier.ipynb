{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms, torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "''' This model is ran on Google colab because I do not have a GPU '''\n",
    "''' You can run these cells and you will have everything necessary to run or load the model'''\n",
    "\n",
    "''' Load the datatset'''\n",
    "def downloadDataset(): \n",
    "    # Download the dataset\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "    return trainset, testset\n",
    "\n",
    "''' Load and umpickle the dataset'''\n",
    "## Cifar-10 dataset comes in batches that need to be combined and unpickled to form full dataset\n",
    "def loadAndUnpickleBatch(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "        images = batch[b'data']\n",
    "        labels = batch[b'labels']\n",
    "        images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    return images, np.array(labels)\n",
    "#Training data is divided into 5 batches\n",
    "def loadAllBatches():\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        path = f'./data/cifar-10-batches-py/data_batch_{i}'\n",
    "        image_batch, label_batch = loadAndUnpickleBatch(path)\n",
    "        images.append(image_batch)\n",
    "        labels.append(label_batch)\n",
    "        combined_images = np.concatenate(images)\n",
    "        combined_labels = np.concatenate(labels)\n",
    "    return combined_images, combined_labels\n",
    "#Test data is in a single batch\n",
    "def loadTestBatch():\n",
    "    path = './data/cifar-10-batches-py/test_batch'\n",
    "    images, labels = loadAndUnpickleBatch(path)\n",
    "    return images, labels\n",
    "\n",
    "''' Sort images and labels into folders '''\n",
    "def sortData(images, labels, folder):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    # Create a folder for each label 0-9\n",
    "    for label in range(10):\n",
    "        label_dir = os.path.join(folder, f'label_{label}')\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "    # Save images into the corresponding label folder\n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        label_dir = os.path.join(folder, f'label_{label}')\n",
    "        image_filename = os.path.join(label_dir, f'image_{idx}.png')\n",
    "        cv2.imwrite(image_filename, image)\n",
    "    print('Data sorted into folders')\n",
    "\n",
    "''' Reduce size of dataset '''\n",
    "def get_first_n_images_per_class(images, labels, n):\n",
    "    selected_images = []\n",
    "    selected_labels = []\n",
    "\n",
    "    for label in range(10):\n",
    "        class_indices = np.where(labels == label)[0][:n]\n",
    "        selected_images.append(images[class_indices])\n",
    "        selected_labels.append(labels[class_indices])\n",
    "    return np.concatenate(selected_images), np.concatenate(selected_labels)\n",
    "\n",
    "def save_cifar10_images_by_label(features, labels, output_folder, n=100):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    label_counts = {}\n",
    "    \n",
    "    for idx, (image, label) in enumerate(zip(features, labels)):\n",
    "        # Convert label to integer to facilitate sorting\n",
    "        label = int(label)\n",
    "        # Skip if we've saved `n` images for this label\n",
    "        if label_counts.get(label, 0) >= n:\n",
    "            continue\n",
    "        # Create a subfolder for the label if it doesn't exist\n",
    "        label_folder = os.path.join(output_folder, str(label))\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "        # Save the image with original index as filename\n",
    "        filename = f\"image_{idx}.png\"\n",
    "        image_path = os.path.join(label_folder, filename)\n",
    "        Image.fromarray(image).save(image_path)\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        # If we've saved `n` images for each label, we're done\n",
    "        if len(label_counts) == len(np.unique(labels)) and all(c >= n for c in label_counts.values()):\n",
    "            break\n",
    "\n",
    "    print(f\"Saved {n} images per label in {output_folder}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ### Download the dataset\n",
    "    trainset, testset = downloadDataset()\n",
    "    ### Load and sort the dataset\n",
    "    # Training\n",
    "    train_images, train_labels = loadAllBatches()\n",
    "    # Test\n",
    "    test_images, test_labels = loadTestBatch()\n",
    "\n",
    "    ## Sort and Reduce size of dataset: training = 500/label, test = 100/label\n",
    "    # Training\n",
    "    save_cifar10_images_by_label(train_images, train_labels, \"Sorted/Training\", n=500)\n",
    "    # Testing    \n",
    "    save_cifar10_images_by_label(test_images, test_labels, \"Sorted/Test\", n=100)\n",
    "\n",
    "    # Sorted dataset will be used as is for CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VGG11(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG11, self).__init__()\n",
    "        \n",
    "        # Define the layers as described\n",
    "        ### Changes are defined in comment form\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=2, stride=1, padding=1),  # Conv(001, 064, 3, 1, 1)\n",
    "            nn.BatchNorm2d(64),  # BatchNorm(064)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # MaxPool(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Conv(064, 128, 3, 1, 1)\n",
    "            nn.BatchNorm2d(128),  # BatchNorm(128)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # MaxPool(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Conv(128, 256, 3, 1, 1)\n",
    "            nn.BatchNorm2d(256),  # BatchNorm(256)\n",
    "            nn.ReLU()  # ReLU activation\n",
    "        )\n",
    "        \n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Conv(256, 256, 3, 1, 1)\n",
    "            nn.BatchNorm2d(256),  # BatchNorm(256)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # MaxPool(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # Conv(256, 512, 3, 1, 1)\n",
    "            ### Changing kernel size to 5\n",
    "            # nn.Conv2d(256, 512, kernel_size=5, stride=1, padding=2),  # Conv(256, 512, 3, 1, 1)\n",
    "            nn.BatchNorm2d(512),  # BatchNorm(512)\n",
    "            nn.ReLU()  # ReLU activation\n",
    "        )\n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # Conv(512, 512, 3, 1, 1)\n",
    "            ### Changing kernel size to 5\n",
    "            # nn.Conv2d(512, 512, kernel_size=5, stride=1, padding=2),  # Conv(512, 512, 3, 1, 1)           \n",
    "            nn.BatchNorm2d(512),  # BatchNorm(512)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # MaxPool(2, 2)\n",
    "        )\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # Conv(512, 512, 3, 1, 1)\n",
    "            nn.BatchNorm2d(512),  # BatchNorm(512)\n",
    "            nn.ReLU()  # ReLU activation\n",
    "        )\n",
    "        \n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # Conv(512, 512, 3, 1, 1)\n",
    "            nn.BatchNorm2d(512),  # BatchNorm(512)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # MaxPool(2, 2)\n",
    "        )\n",
    "        # ### Added layer\n",
    "        # self.conv_block9 = nn.Sequential(\n",
    "        #     nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # Conv(512, 512, 3, 1, 1)\n",
    "        #     nn.BatchNorm2d(512),  # BatchNorm(512)\n",
    "        #     nn.ReLU(),  # ReLU activation\n",
    "        #     ## no max pool else size is too small\n",
    "        # )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),  # Linear(512, 4096)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.Dropout(0.5),  # Dropout(0.5)\n",
    "            nn.Linear(4096, 4096),  # Linear(4096, 4096)\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.Dropout(0.5),  # Dropout(0.5)\n",
    "            nn.Linear(4096, num_classes)  # Linear(4096, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through all convolutional blocks\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        x = self.conv_block6(x)\n",
    "        x = self.conv_block7(x)\n",
    "        x = self.conv_block8(x)\n",
    "        # x = self.conv_block9(x)\n",
    "\n",
    "        # Flatten the output from the convolutional layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 512*1*1)\n",
    "        \n",
    "        # Forward pass through fully connected layers\n",
    "        x = self.fc_block(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Rum to train\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #set device to amd gpu if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    ''' Import data and preprocess to input into model '''\n",
    "    # We are using the sorted dataset as is which is why the preprocessing is done here   \n",
    "\n",
    "    # Define preprocessing for train and test datasets\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor (C x H x W)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    # Load train and test datasets\n",
    "    train_dataset = datasets.ImageFolder(root=\"Sorted/Training\", transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=\"Sorted/Test\", transform=transform)\n",
    "    # Create data loaders for training and testing to use in PyTorch\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize the VGG11 model\n",
    "    model = VGG11(num_classes=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # Ensure it is being run on GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Training loop\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        # Print the training accuracy and loss\n",
    "        print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    #Evaluate the model on the test set and get accuracy\n",
    "    model.eval()\n",
    "    correctlyClassified = 0\n",
    "    totalSamples = 0\n",
    "    # Disable gradient tracking because we don't need it for evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            totalSamples += labels.size(0)\n",
    "            correctlyClassified += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correctlyClassified / totalSamples:.2f}%\")   \n",
    "\n",
    "    # Saving the model\n",
    "    torch.save(model, 'CNNclassifier.pth')  # Save the entire model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_vgg11_with_confusion_matrix(model, test_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move inputs and targets to the same device as the model, running on gpu\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Get predictions from the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            # Append predictions and true labels to the lists\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    return cm\n",
    "\n",
    "def evalModel(model):\n",
    "    # Evaluate on test set accuracy\n",
    "    model.eval()\n",
    "    correctlyClassified = 0\n",
    "    totalSamples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            totalSamples += labels.size(0)\n",
    "            correctlyClassified += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correctlyClassified / totalSamples:.2f}%\")\n",
    "\n",
    "def evaluate_cnn_model(model, test_loader, device):\n",
    "# Evaluating model to get all metrics for table\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move inputs and targets to the same device as the model\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Get predictions from the model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            # Append predictions and true labels to the lists\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "\n",
    "    # Get classification metrics using sklearn\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "\n",
    "    # Extract precision, recall, f1 score for each class and average\n",
    "    class_metrics = {class_label: {\n",
    "        'precision': report[str(class_label)]['precision'],\n",
    "        'recall': report[str(class_label)]['recall'],\n",
    "        'f1-score': report[str(class_label)]['f1-score']\n",
    "    } for class_label in range(10)}\n",
    "\n",
    "    # Extract average metrics (macro avg, weighted avg)\n",
    "    avg_metrics = {\n",
    "        'precision': report['accuracy'],  # Accuracy is used as precision for the 'accuracy' metric\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1-score': report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Return metrics as a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'class_metrics': class_metrics,\n",
    "        'average_metrics': avg_metrics,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_average_metrics(models, model_names, test_loader, device):\n",
    "    all_metrics = []  # List to hold average metrics for all models\n",
    "    ## Get all metrics for each model in table\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        model.to(device)\n",
    "        metrics = evaluate_cnn_model(model, test_loader, device)\n",
    "        \n",
    "        # Extract average metrics\n",
    "        avg_metrics = metrics['average_metrics']\n",
    "        accuracy = metrics['accuracy']\n",
    "\n",
    "        # Add model name, average metrics, and accuracy\n",
    "        model_metrics = {\n",
    "            'Model Name': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': avg_metrics['precision'],\n",
    "            'Recall': avg_metrics['recall'],\n",
    "            'F1-Score': avg_metrics['f1-score']\n",
    "        }\n",
    "\n",
    "        # Append model's metrics to the list\n",
    "        all_metrics.append(model_metrics)\n",
    "\n",
    "    # Create DataFrame for all models' average metrics\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    return metrics_df\n",
    "\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    #set device to amd gpu if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    ''' Import data and preprocess to input into model '''\n",
    "    # We are using the sorted dataset as is which is why the preprocessing is done here\n",
    "    # Define preprocessing for train and test datasets\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    # Load train and test datasets\n",
    "    train_dataset = datasets.ImageFolder(root=\"Sorted/Training\", transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=\"Sorted/Test\", transform=transform)\n",
    "    # Create data loaders for training and testing to use in PyTorch\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "    ''' Loading and Evaluating models'''\n",
    "    ### Evaluating all CNN models\n",
    "    CNNclassifier = VGG11(num_classes=10)\n",
    "    CNNclassifier = torch.load('CNNclassifier.pth')  # Load the entire model\n",
    "    evalModel(CNNclassifier)\n",
    "    confusion_mat = evaluate_vgg11_with_confusion_matrix(CNNclassifier, test_loader, device)\n",
    "\n",
    "    CNNclassifierAddLayer = VGG11(num_classes=10)\n",
    "    CNNclassifierAddLayer = torch.load('CNNclassifierAddLayer.pth')  # Load the entire model\n",
    "    evalModel(CNNclassifierAddLayer)\n",
    "    confusion_mat = evaluate_vgg11_with_confusion_matrix(CNNclassifierAddLayer, test_loader, device)\n",
    "\n",
    "    CNNclassifierChangeKernelsize5 = VGG11(num_classes=10)\n",
    "    CNNclassifierChangeKernelsize5 = torch.load('CNNclassifierChangeKernelsize5.pth')  # Load the entire model\n",
    "    evalModel(CNNclassifierChangeKernelsize5)\n",
    "    confusion_mat = evaluate_vgg11_with_confusion_matrix(CNNclassifierChangeKernelsize5, test_loader, device)\n",
    "\n",
    "    CNNclassifierChangeKernelsize5_6 = VGG11(num_classes=10)\n",
    "    CNNclassifierChangeKernelsize5_6 = torch.load('CNNclassifierChangeKernelsize5_6.pth')  # Load the entire model\n",
    "    evalModel(CNNclassifierChangeKernelsize5_6)\n",
    "    confusion_mat = evaluate_vgg11_with_confusion_matrix(CNNclassifierChangeKernelsize5_6, test_loader, device)\n",
    "\n",
    "    CNNclassifierDoubleEpochs = VGG11(num_classes=10)\n",
    "    CNNclassifierDoubleEpochs = torch.load('CNNclassifierDoubleEpochs.pth')  # Load the entire model\n",
    "    evalModel(CNNclassifierDoubleEpochs)\n",
    "    confusion_mat = evaluate_vgg11_with_confusion_matrix(CNNclassifierDoubleEpochs, test_loader, device)\n",
    "\n",
    "    # Display table with all metrics\n",
    "    models = [CNNclassifier, CNNclassifierAddLayer, CNNclassifierChangeKernelsize5,CNNclassifierChangeKernelsize5_6,CNNclassifierDoubleEpochs]\n",
    "    model_names = [\"CNNclassifier\", \"CNNclassifierAddLayer\", \"CNNclassifierChangeKernelsize5\",\"CNNclassifierChangeKernelsize5_6\",\"CNNclassifierDoubleEpochs\"]\n",
    "\n",
    "    # Evaluate the models and get a table with average metrics\n",
    "    comparison_df = evaluate_average_metrics(models,model_names, test_loader, device)\n",
    "    print(comparison_df)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
